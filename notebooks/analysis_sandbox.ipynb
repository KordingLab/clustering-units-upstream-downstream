{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collaborative-interference",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/rlange/modularity-io/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complimentary-bernard",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.lines import Line2D\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from analysis import load_data_as_table, generate_model_specs\n",
    "from modularity import sort_by_cluster, girvan_newman\n",
    "import itertools\n",
    "from util import merge_dicts\n",
    "from models import LitWrapper\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from colorsys import hsv_to_rgb\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Record of the experiments we ran and hyperparameters used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = {\n",
    "    'experiment-1': {\n",
    "        'log-dir-prefix': 'mnist-base',\n",
    "        'dataset': 'mnist',\n",
    "        'regularization': {\n",
    "            'l2': np.logspace(-5,-1,9),\n",
    "            'l1': np.logspace(-5,-2,7),\n",
    "            'drop': np.linspace(0.05, 0.7, 14)\n",
    "        },\n",
    "        'seeds': 9,\n",
    "        'layers': 2\n",
    "    },\n",
    "    'experiment-2': {\n",
    "        'log-dir-prefix': 'mnist-wide',\n",
    "        'dataset': 'mnist',\n",
    "        'regularization': {\n",
    "            'l2': np.logspace(-5,-1,5),\n",
    "            'l1': np.logspace(-5,-2,4),\n",
    "            'drop': np.linspace(0.1, 0.6, 6)\n",
    "        },\n",
    "        'seeds': 3,\n",
    "        'layers': 2\n",
    "    },\n",
    "    'experiment-3': {\n",
    "        'log-dir-prefix': 'mnist-deep',\n",
    "        'dataset': 'mnist',\n",
    "        'regularization': {\n",
    "            'l2': np.logspace(-5,-1,5),\n",
    "            'l1': np.logspace(-5,-2,4),\n",
    "            'drop': np.linspace(0.1, 0.6, 6)\n",
    "        },\n",
    "        'seeds': 3,\n",
    "        'layers': 5\n",
    "    },\n",
    "    'cifar10': {\n",
    "        'dataset': 'cifar10',\n",
    "        'log-dir-prefix': 'cifar10',\n",
    "        'regularization': {\n",
    "            'l2': np.logspace(-5,-1,5),\n",
    "            'l1': np.logspace(-5,-2,4),\n",
    "            'drop': np.linspace(0.1, 0.5, 5)\n",
    "        },\n",
    "        'seeds': 3,\n",
    "        'layers': 6\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "choose one of the above keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = 'experiment-1'\n",
    "config = experiments[experiment]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "instructional-leone",
   "metadata": {},
   "source": [
    "# Basic metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "documented-wayne",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['train_loss', 'val_loss', 'test_loss', 'train_acc', 'val_acc', 'test_acc', 'l1_norm', 'l2_norm', 'sparsity', 'nuc_norm']\n",
    "test_acc_cutoff = 0.8\n",
    "\n",
    "basic_df_l2 = load_data_as_table(\n",
    "    generate_model_specs({'dataset': config['dataset'], 'task': 'sup', 'l1': 0.0, 'drop': 0.0,\n",
    "                          'l2': config['regularization']['l2'], 'run': range(config['seeds']), 'checkpoint': 'best'}),\n",
    "    metrics,\n",
    "    log_dir=Path(f'logs-{config[\"log-dir-prefix\"]}-l2'))\n",
    "\n",
    "basic_df_l1 = load_data_as_table(\n",
    "    generate_model_specs({'dataset': config['dataset'], 'task': 'sup', 'l2': 1e-5, 'drop': 0.0,\n",
    "                          'l1': config['regularization']['l1'], 'run': range(config['seeds']), 'checkpoint': 'best'}),\n",
    "    metrics,\n",
    "    log_dir=Path(f'logs-{config[\"log-dir-prefix\"]}-l1'))\n",
    "\n",
    "basic_df_drop = load_data_as_table(\n",
    "    generate_model_specs({'dataset': config['dataset'], 'task': 'sup', 'l2': 1e-5, 'drop': config['regularization']['drop'],\n",
    "                          'l1': 0., 'run': range(config['seeds']), 'checkpoint': 'best'}),\n",
    "    metrics,\n",
    "    log_dir=Path(f'logs-{config[\"log-dir-prefix\"]}-drop'))\n",
    "\n",
    "basic_df_null = load_data_as_table(\n",
    "    generate_model_specs({'dataset': config['dataset'], 'task': 'sup', 'l2': 0., 'drop': 0., 'l1': 0.,\n",
    "                          'run': range(1000,1100), 'checkpoint': 'dummy.ckpt'}),\n",
    "    metrics,\n",
    "    log_dir=Path(f'logs-{config[\"log-dir-prefix\"]}-dummy'))\n",
    "\n",
    "basic_df_all = pd.concat([basic_df_l2, basic_df_l1, basic_df_drop])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Figure out how much regularization is too much regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def max_reg(df, field):\n",
    "    last_val = None\n",
    "    for val in sorted(df[field].unique()):\n",
    "        pct_bad = (df[df[field] == val]['test_acc'] < test_acc_cutoff).mean()\n",
    "        if pct_bad > 0.5:\n",
    "            return last_val\n",
    "        last_val = val\n",
    "    return val\n",
    "\n",
    "\n",
    "max_l2 = max_reg(basic_df_l2, 'l2')\n",
    "max_l1 = max_reg(basic_df_l1, 'l1')\n",
    "max_drop = max_reg(basic_df_drop, 'drop')\n",
    "\n",
    "print(f\"Keeping L2 ≤ {max_l2}\")\n",
    "print(f\"Keeping L1 ≤ {max_l1}\")\n",
    "print(f\"Keeping drop ≤ {max_drop}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reflected-eagle",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def df_slice(df:pd.DataFrame, condition:dict):\n",
    "    for key, val in condition.items():\n",
    "        if val is None or (isinstance(val, float) and np.isnan(val)):\n",
    "            df = df[np.isnan(df[key])]\n",
    "        else:\n",
    "            df = df[df[key] == val]\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_vs_hyper_by_run(df, x, y, ax=None, line_args={}, errbar_args={}, df_null=None):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    if df_null is not None:\n",
    "        y_null = df_null[y]\n",
    "        null_mean = y_null.mean()\n",
    "        null_std = y_null.std()\n",
    "        xlims = [df[x].min(), df[x].max()]\n",
    "        for sigma in [1,2,3]:\n",
    "            ax.fill_between(xlims, null_mean-sigma*null_std, null_mean+sigma*null_std,\n",
    "                            color=(0.,0.,0.,0.1), edgecolor='none')\n",
    "        ax.plot(xlims, [null_mean, null_mean], color=(.5,.5,.5))\n",
    "    \n",
    "    rgroups = df.groupby('run')\n",
    "    for idx, grp in rgroups:\n",
    "        grp.plot(x, y, ax=ax, **line_args)\n",
    "    \n",
    "    mu = df.groupby(x).mean()\n",
    "    sem = df.groupby(x).agg(lambda x: x.std() / np.sqrt(x.count()))\n",
    "    mu.plot(y=y, yerr=sem[y], ax=ax, ylabel=y, xlabel=x, **errbar_args)\n",
    "    \n",
    "    if errbar_args.get('legend', False):\n",
    "        legend_handles = []\n",
    "        legend_labels = []\n",
    "        for splt in errbar_splits:\n",
    "            if splt == 'assoc':\n",
    "                items = ['forward', 'backward', 'cov/hess', 'jacobian', 'raw', 'norm']\n",
    "            else:\n",
    "                items = df[splt].unique()\n",
    "            for v in items:\n",
    "                legend_handles.append(Line2D([0], [0], **plot_format({splt: v})))\n",
    "                legend_labels.append(str(v))\n",
    "        plt.legend(legend_handles, legend_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-patch",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(6,3,figsize=(10, 12))\n",
    "\n",
    "## TRAIN ACC ##\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l1':0.,'drop':0.}),\n",
    "                     x='l2', y='train_acc', ax=ax[0,0],\n",
    "                     line_args={'linewidth': 0.5, 'logx': True, 'legend':False, 'ylim': [test_acc_cutoff, 1.]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l2':1e-5,'drop':0.}),\n",
    "                     x='l1', y='train_acc', ax=ax[0,1],\n",
    "                     line_args={'linewidth': 0.5, 'logx': True, 'legend':False, 'ylim': [test_acc_cutoff, 1.]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l1':0.,'l2':1e-5}),\n",
    "                     x='drop', y='train_acc', ax=ax[0,2],\n",
    "                     line_args={'linewidth': 0.5, 'logx': False, 'legend':False, 'ylim': [test_acc_cutoff, 1.]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "\n",
    "## TEST ACC ##\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l1':0.,'drop':0.}),\n",
    "                     x='l2', y='test_acc', ax=ax[1,0],\n",
    "                     line_args={'linewidth': 0.5, 'logx': True, 'legend':False, 'ylim': [test_acc_cutoff, 1.]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l2':1e-5,'drop':0.}),\n",
    "                     x='l1', y='test_acc', ax=ax[1,1],\n",
    "                     line_args={'linewidth': 0.5, 'logx': True, 'legend':False, 'ylim': [test_acc_cutoff, 1.]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l1':0.,'l2':1e-5}),\n",
    "                     x='drop', y='test_acc', ax=ax[1,2],\n",
    "                     line_args={'linewidth': 0.5, 'logx': False, 'legend':False, 'ylim': [test_acc_cutoff, 1.]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "\n",
    "## L2 WEIGHT NORM ##\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l1':0.,'drop':0.}),\n",
    "                     x='l2', y='l2_norm', ax=ax[2,0],\n",
    "                     line_args={'linewidth': 0.5, 'logx': True, 'legend':False, 'ylim': [1, basic_df_all['l2_norm'].max()]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l2':1e-5,'drop':0.}),\n",
    "                     x='l1', y='l2_norm', ax=ax[2,1],\n",
    "                     line_args={'linewidth': 0.5, 'logx': True, 'legend':False, 'ylim': [1, basic_df_all['l2_norm'].max()]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l1':0.,'l2':1e-5}),\n",
    "                     x='drop', y='l2_norm', ax=ax[2,2],\n",
    "                     line_args={'linewidth': 0.5, 'logx': False, 'legend':False, 'ylim': [1, basic_df_all['l2_norm'].max()]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "\n",
    "## L1 WEIGHT NORM ##\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l1':0.,'drop':0.}),\n",
    "                     x='l2', y='l1_norm', ax=ax[3,0],\n",
    "                     line_args={'linewidth': 0.5, 'logx': True, 'legend':False, 'ylim': [1, basic_df_all['l1_norm'].max()]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l2':1e-5,'drop':0.}),\n",
    "                     x='l1', y='l1_norm', ax=ax[3,1],\n",
    "                     line_args={'linewidth': 0.5, 'logx': True, 'legend':False, 'ylim': [1, basic_df_all['l1_norm'].max()]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l1':0.,'l2':1e-5}),\n",
    "                     x='drop', y='l1_norm', ax=ax[3,2],\n",
    "                     line_args={'linewidth': 0.5, 'logx': False, 'legend':False, 'ylim': [1, basic_df_all['l1_norm'].max()]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "\n",
    "## SPARSITY ##\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l1':0.,'drop':0.}),\n",
    "                     x='l2', y='sparsity', ax=ax[4,0],\n",
    "                     line_args={'linewidth': 0.5, 'logx': True, 'legend':False, 'ylim': [0, 1]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l2':1e-5,'drop':0.}),\n",
    "                     x='l1', y='sparsity', ax=ax[4,1],\n",
    "                     line_args={'linewidth': 0.5, 'logx': True, 'legend':False, 'ylim': [0, 1]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l1':0.,'l2':1e-5}),\n",
    "                     x='drop', y='sparsity', ax=ax[4,2],\n",
    "                     line_args={'linewidth': 0.5, 'logx': False, 'legend':False, 'ylim': [0, 1]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "\n",
    "## NUCLEAR NORM ##\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l1':0.,'drop':0.}),\n",
    "                     x='l2', y='nuc_norm', ax=ax[5,0],\n",
    "                     line_args={'linewidth': 0.5, 'logx': True, 'legend':False, 'ylim': [0, basic_df_all['nuc_norm'].max()]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l2':1e-5,'drop':0.}),\n",
    "                     x='l1', y='nuc_norm', ax=ax[5,1],\n",
    "                     line_args={'linewidth': 0.5, 'logx': True, 'legend':False, 'ylim': [0, basic_df_all['nuc_norm'].max()]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "plot_vs_hyper_by_run(df_slice(basic_df_all, {'l1':0.,'l2':1e-5}),\n",
    "                     x='drop', y='nuc_norm', ax=ax[5,2],\n",
    "                     line_args={'linewidth': 0.5, 'logx': False, 'legend':False, 'ylim': [0, basic_df_all['nuc_norm'].max()]},\n",
    "                     errbar_args={'fmt': '-k', 'linewidth': 1.5, 'legend': False},\n",
    "                     df_null=basic_df_null)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"figures/{config['log-dir-prefix'].replace('-', '_')}_performance_and_norms.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "earned-instruction",
   "metadata": {},
   "source": [
    "# Load and plot some association matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alike-brother",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_spec = {'dataset': config['dataset'], 'task': 'sup', 'l2': 1e-5, 'l1':0.0, 'drop': 0.3, 'run': 0, 'layer': 0}\n",
    "demo_ckpt = f'logs-{config[\"log-dir-prefix\"]}-drop/{LitWrapper(**model_spec).get_uid()}/weights/best.ckpt'\n",
    "\n",
    "info = torch.load(demo_ckpt)\n",
    "mods = info[\"modules\"]\n",
    "\n",
    "def plot_assoc(A, ax=None, lines_at=None, line_args={}, colorbar=False, vmin=0., vmax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    mat = ax.matshow(A, vmin=vmin, vmax=vmax)\n",
    "    if colorbar:\n",
    "        plt.colorbar(mat, ax=ax)\n",
    "    if lines_at is not None:\n",
    "        sz = len(A)\n",
    "        line_args = merge_dicts({'color': 'r', 'linewidth':0.5}, line_args)\n",
    "        for x in np.cumsum(lines_at)-0.5:\n",
    "            ax.plot([-.5, sz-.5], [x, x], **line_args)\n",
    "            ax.plot([x, x], [-.5, sz-.5], **line_args)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constitutional-workplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "assoc_to_plot = ['forward_jac', 'backward_jac']\n",
    "\n",
    "\n",
    "relabel = {\n",
    "    'backward_hess': 'output hessian',\n",
    "    'backward_hess_norm': 'output hessian (norm)',\n",
    "    'backward_jac': 'output gradient',\n",
    "    'backward_jac_norm': 'output gradient (norm)',\n",
    "    'forward_cov': 'input covariance',\n",
    "    'forward_cov_norm': 'input covariance (norm)',\n",
    "    'forward_jac': 'input gradient',\n",
    "    'forward_jac_norm': 'input gradient (norm)',\n",
    "    'l2': '$L_2$',\n",
    "    'l1': '$L_1$',\n",
    "    'drop': 'drop',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-bride",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, len(assoc_to_plot), figsize=(3*len(assoc_to_plot), 3))\n",
    "for i, a in enumerate(assoc_to_plot):\n",
    "    adj, clust = mods[a][model_spec['layer']]['adj'].numpy(), mods[a][model_spec['layer']]['clusters'].numpy()\n",
    "    adj = adj / adj.sum()\n",
    "    plot_assoc(adj, ax[i])\n",
    "    ax[i].set_title(relabel[a])\n",
    "plt.suptitle(\"     \".join(f\"{k}={model_spec[k]}\" for k in ['l2', 'l1', 'drop', 'run']))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overall-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(8, 3*len(assoc_to_plot)))\n",
    "grid = GridSpec(len(assoc_to_plot), 16)\n",
    "\n",
    "for i, a in enumerate(assoc_to_plot):\n",
    "    # Plot assoc and clusters (unsorted)\n",
    "    assoc_ax, clust_ax = fig.add_subplot(grid[i, 0:5]), fig.add_subplot(grid[i, 5:8])\n",
    "    adj, clust = mods[a][model_spec['layer']]['adj'].numpy(), mods[a][model_spec['layer']]['clusters'].numpy()\n",
    "\n",
    "    plot_assoc(adj, ax=assoc_ax)\n",
    "    assoc_ax.set_title(relabel[a])\n",
    "    \n",
    "    clust_ax.matshow(1-clust[:, :8], cmap='gray')\n",
    "    clust_ax.set_xticks([]); clust_ax.set_yticks([])\n",
    "    clust_ax.set_title('Clusters')\n",
    "    \n",
    "    # Plot assoc and clusters (sorted)\n",
    "    isrt = sort_by_cluster(clust)\n",
    "    assoc_ax, clust_ax = fig.add_subplot(grid[i, 8:13]), fig.add_subplot(grid[i, 13:16])\n",
    "    isrt = sort_by_cluster(clust, remove_dead=True)\n",
    "\n",
    "    plot_assoc(adj[isrt,:][:,isrt], ax=assoc_ax, lines_at=clust.sum(0))\n",
    "    assoc_ax.set_title(relabel[a]+\" (sorted)\")\n",
    "    \n",
    "    clust_ax.matshow(1-clust[isrt, :8], cmap='gray')\n",
    "    clust_ax.set_xticks([]); clust_ax.set_yticks([])\n",
    "    clust_ax.set_title('Clusters (sorted)')\n",
    "plt.suptitle(\"     \".join(f\"{k}={model_spec[k]}\" for k in ['l2', 'l1', 'run']))\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "danish-briefing",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(assoc_to_plot), len(assoc_to_plot), figsize=(4*len(assoc_to_plot), 4*len(assoc_to_plot)))\n",
    "for i, ai in enumerate(assoc_to_plot):\n",
    "    adj_i, clust_i = mods[ai][model_spec['layer']]['adj'].numpy(), mods[ai][model_spec['layer']]['clusters'].numpy()\n",
    "    adj_i = adj_i / adj_i.sum()\n",
    "    sizes_i = clust_i.sum(axis=0)\n",
    "    sort_i = np.argsort(np.argmax(clust_i, axis=1))\n",
    "    # remove dead units from sorting indices\n",
    "    sort_i = np.array([idx for idx in sort_i if not clust_i[idx,:].sum()==0.])\n",
    "\n",
    "    for j, aj in enumerate(assoc_to_plot):\n",
    "        adj_j = mods[aj][model_spec['layer']]['adj'].numpy()\n",
    "        adj_j = adj_j / adj_j.sum()\n",
    "        plot_assoc(adj_j[sort_i,:][:,sort_i], lines_at=sizes_i, ax=ax[i,j])\n",
    "        ax[i,j].set_title(relabel[ai]+\"[\"+relabel[aj]+\"]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interested-evolution",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(len(assoc_to_plot), len(assoc_to_plot), figsize=(4*len(assoc_to_plot), 4*len(assoc_to_plot)))\n",
    "for i, ai in enumerate(assoc_to_plot):\n",
    "    clust_i = mods[ai][model_spec['layer']]['clusters'].numpy()\n",
    "    sort_i = sort_by_cluster(clust_i, remove_dead=True)\n",
    "\n",
    "    for j, aj in enumerate(assoc_to_plot):\n",
    "        clust_j = mods[aj][model_spec['layer']]['clusters'].numpy()\n",
    "        ax[i,j].matshow(1-clust_j[sort_i, :8], cmap='gray')\n",
    "        ax[i,j].set_xticks([]); clust_ax.set_yticks([])\n",
    "        ax[i,j].set_title(relabel[ai]+\"[\"+relabel[aj]+\"]\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weighted-frequency",
   "metadata": {},
   "source": [
    "# Modularity series plotting helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brilliant-tactics",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _isinput(assoc:str):\n",
    "    return ('forward' in assoc) or ('input' in assoc)\n",
    "\n",
    "def _isgrad(assoc:str):\n",
    "    return ('jac' in assoc) or ('grad' in assoc)\n",
    "\n",
    "def _isnorm(assoc:str):\n",
    "    return '_norm' in assoc\n",
    "\n",
    "\n",
    "def regularization_amount(hyp:str, val):\n",
    "    if hyp == 'l2':\n",
    "        return (np.log(val) - np.log(min(config['regularization']['l2']))) / (np.log(max(config['regularization']['l2'])) - np.log(min(config['regularization']['l2'])))\n",
    "    elif hyp == 'l1':\n",
    "        return (np.log(val) - np.log(min(config['regularization']['l1']))) / (np.log(max(config['regularization']['l1'])) - np.log(min(config['regularization']['l1'])))\n",
    "    elif hyp == 'drop':\n",
    "        return val\n",
    "\n",
    "\n",
    "def p_string(pvalue, n_comparisons=1):\n",
    "    if pvalue < 0.001/n_comparisons:\n",
    "        return '***'\n",
    "    elif pvalue < 0.01/n_comparisons:\n",
    "        return '**'\n",
    "    elif pvalue < 0.05/n_comparisons:\n",
    "        return '*'\n",
    "    else:\n",
    "        return ''\n",
    "\n",
    "\n",
    "def plot_format(group_identifier: dict, seed_hue=1/6):\n",
    "    h, s, v, marker = seed_hue, 0.9, 0.5, '.'\n",
    "    for k, v in group_identifier.items():\n",
    "        if k == 'assoc':\n",
    "            h = seed_hue if _isinput(v) else seed_hue-1/4\n",
    "            h += (+1/4 if _isinput(v) else -1/4) * _isgrad(v)\n",
    "            v = 0.5 if _isnorm(v) else 0.9\n",
    "        elif k == 'layer':\n",
    "            marker = 'o^'[v]\n",
    "    return {'marker': marker, 'color': hsv_to_rgb(h%1.0,s,v)}\n",
    "\n",
    "\n",
    "def plot_vs_hyper_splits(df, x, y, std_hyper=False, ax=None, errbar_splits=['assoc'], errbar_args={}, df_null=None, null_color=(0.,0.,0.), null_x=False):\n",
    "    ax = ax or plt.gca()\n",
    "    \n",
    "    if std_hyper:\n",
    "        df[x] = regularization_amount(x, df[x])\n",
    "        x_name = 'reg. amt'\n",
    "    else:\n",
    "        x_name = x\n",
    "        \n",
    "    if df_null is not None:\n",
    "        if null_x:\n",
    "            null_x_vals, null_means, null_stds = np.array([]), np.array([]), np.array([])\n",
    "            for idx, grp in df_null.groupby(x):\n",
    "                null_x_vals = np.append(null_x_vals, idx)\n",
    "                null_means = np.append(null_means, grp[y].mean())\n",
    "                null_stds = np.append(null_stds, grp[y].std())\n",
    "        else:\n",
    "            y_null = df_null[y]\n",
    "            null_means = np.array([y_null.mean()] * 2)\n",
    "            null_stds = np.array([y_null.std()] * 2)\n",
    "            null_x_vals = [df[x].min(), df[x].max()]\n",
    "            \n",
    "        for sigma in [1,2,3]:\n",
    "            ax.fill_between(null_x_vals, null_means-sigma*null_stds, null_means+sigma*null_stds,\n",
    "                            color=null_color+(0.1,), edgecolor='none')\n",
    "        ax.plot(null_x_vals, null_means, color=null_color)\n",
    "    \n",
    "    for idx, grp in df.groupby(errbar_splits):\n",
    "        args = {'xlabel': x_name, 'ylabel': y, 'label': str(idx)}\n",
    "        args.update(errbar_args)\n",
    "        t_idx = (idx,) if not isinstance(idx, tuple) else idx\n",
    "        mu = grp.groupby(x).mean()\n",
    "        sem = grp.groupby(x).agg(lambda x: x.std() / np.sqrt(x.count()))\n",
    "        mu.plot(y=y, yerr=sem[y], ax=ax, **merge_dicts(plot_format(dict(zip(errbar_splits, t_idx))), args))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-concord",
   "metadata": {},
   "source": [
    "Make a cube legend showing 2x2x2 splits between association methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "undefined-rehabilitation",
   "metadata": {},
   "source": [
    "# Modularity metrics vs hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clinical-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assoc = [\"forward_cov\", \"backward_hess\", \"forward_jac\", \"backward_jac\"]\n",
    "assoc = [\"forward_cov\", \"forward_jac\", \"backward_jac\"]\n",
    "norms = [\"\", \"_norm\"]\n",
    "all_assoc = [f\"{io+nm}\" for (io,nm) in itertools.product(assoc,norms)]\n",
    "fields = (\"score\", \"num_clusters\", \"adj\")\n",
    "layers = list(range(config['layers']))\n",
    "mod_metrics = [f\"modules.{a}.{lay}.{field}\" for (a, lay, field) in itertools.product(all_assoc, layers, fields)]\n",
    "mod_metrics.append(\"test_acc\")\n",
    "\n",
    "mod_df_l2 = load_data_as_table(\n",
    "    generate_model_specs({'dataset': config['dataset'], 'task': 'sup', 'l1': 0.0, 'drop': 0.0,\n",
    "                          'l2': config['regularization']['l2'], 'run': range(config['seeds']), 'checkpoint': 'best'}),\n",
    "    mod_metrics,\n",
    "    log_dir=Path(f'logs-{config[\"log-dir-prefix\"]}-l2'))\n",
    "\n",
    "mod_df_l1 = load_data_as_table(\n",
    "    generate_model_specs({'dataset': config['dataset'], 'task': 'sup', 'l2': 1e-5, 'drop': 0.0,\n",
    "                          'l1': config['regularization']['l1'], 'run': range(config['seeds']), 'checkpoint': 'best'}),\n",
    "    mod_metrics,\n",
    "    log_dir=Path(f'logs-{config[\"log-dir-prefix\"]}-l1'))\n",
    "\n",
    "mod_df_drop = load_data_as_table(\n",
    "    generate_model_specs({'dataset': config['dataset'], 'task': 'sup', 'l2': 1e-5, 'drop': config['regularization']['drop'],\n",
    "                          'l1': 0.0, 'run': range(config['seeds']), 'checkpoint': 'best'}),\n",
    "    mod_metrics,\n",
    "    log_dir=Path(f'logs-{config[\"log-dir-prefix\"]}-drop'))\n",
    "\n",
    "mod_df_null = load_data_as_table(\n",
    "    generate_model_specs({'dataset': config['dataset'], 'task': 'sup', 'l2': 0.0, 'drop': 0.0, 'l1': 0.0, 'run': range(1000, 1100), 'checkpoint': 'dummy.ckpt'}),\n",
    "    mod_metrics,\n",
    "    log_dir=Path(f'logs-{config[\"log-dir-prefix\"]}-dummy'))\n",
    "\n",
    "mod_df_all = pd.concat([mod_df_l2, mod_df_l1, mod_df_drop])\n",
    "\n",
    "# Drop modularity metrics wherever test_acc < test_acc_cutoff\n",
    "mod_df_l2.loc[mod_df_l2[\"test_acc\"] < test_acc_cutoff, fields] = float('nan')\n",
    "mod_df_l1.loc[mod_df_l1[\"test_acc\"] < test_acc_cutoff, fields] = float('nan')\n",
    "mod_df_drop.loc[mod_df_drop[\"test_acc\"] < test_acc_cutoff, fields] = float('nan')\n",
    "mod_df_all.loc[mod_df_all[\"test_acc\"] < test_acc_cutoff, fields] = float('nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "apart-thanks",
   "metadata": {},
   "source": [
    "Brief aside to inspect histogram of raw association scores ($\\mathbf{A}$ in the paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-tennessee",
   "metadata": {},
   "outputs": [],
   "source": [
    "msize = 20\n",
    "fig = plt.figure(figsize=(3,3))\n",
    "ax = fig.add_subplot(1,1,1,projection='3d')\n",
    "xyz = list(itertools.product((0,1),(0,1),(0,1)))\n",
    "for (x1,y1,z1),(x2,y2,z2) in itertools.product(xyz,xyz):\n",
    "    if abs(x2-x1)+abs(y2-y1)+abs(z2-z1)==1:\n",
    "        plt.plot((x1,x2),(y1,y2),(z1,z2),'-k')\n",
    "ax.plot(0,0,0,**merge_dicts(plot_format({'assoc':'forward_cov'}), {'marker':'o','markersize':msize}))\n",
    "ax.plot(1,0,0,**merge_dicts(plot_format({'assoc':'backward_hess'}), {'marker':'o','markersize':msize}))\n",
    "ax.plot(0,1,0,**merge_dicts(plot_format({'assoc':'forward_jac'}), {'marker':'o','markersize':msize}))\n",
    "ax.plot(1,1,0,**merge_dicts(plot_format({'assoc':'backward_jac'}), {'marker':'o','markersize':msize}))\n",
    "ax.plot(0,0,1,**merge_dicts(plot_format({'assoc':'forward_cov_norm'}), {'marker':'o','markersize':msize}))\n",
    "ax.plot(1,0,1,**merge_dicts(plot_format({'assoc':'backward_hess_norm'}), {'marker':'o','markersize':msize}))\n",
    "ax.plot(0,1,1,**merge_dicts(plot_format({'assoc':'forward_jac_norm'}), {'marker':'o','markersize':msize}))\n",
    "ax.plot(1,1,1,**merge_dicts(plot_format({'assoc':'backward_jac_norm'}), {'marker':'o','markersize':msize}))\n",
    "ax.set_xticks([]); ax.set_xlabel('input   output')\n",
    "ax.set_yticks([]); ax.set_ylabel('covariance   gradient')\n",
    "ax.set_zticks([]); ax.set_zlabel('unnorm.   norm.')\n",
    "plt.show()\n",
    "\n",
    "for a in all_assoc:\n",
    "    print(a, \"#\"+\"\".join([\"%02X\"%int(255.*val) for val in plot_format({'assoc':a})['color']]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "removed-handy",
   "metadata": {},
   "source": [
    "### Inspect modularity metrics' distribution in untrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afraid-gibson",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(2,4,figsize=(12,6))\n",
    "for i, a in enumerate(all_assoc):\n",
    "    df_slice(mod_df_null, {'assoc':a}).groupby('layer')\\\n",
    "        .hist(column='score', ax=ax[i//4,i%4], alpha=0.3, bins=np.linspace(.05,.15,21), density=True)\n",
    "    ax[i//4,i%4].set_title(a)\n",
    "ax[0,0].legend([f'layer $h^{i+1}$' for i in layers])\n",
    "fig.suptitle('Null distribution of modularity scores')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig,ax = plt.subplots(2,4,figsize=(12,6))\n",
    "for i, a in enumerate(all_assoc):\n",
    "    df_slice(mod_df_null, {'assoc':a}).groupby('layer')\\\n",
    "        .hist(column='num_clusters', ax=ax[i//4,i%4], alpha=0.3, bins=np.linspace(0.,10.,21), density=True)\n",
    "    ax[i//4,i%4].set_title(a)\n",
    "ax[0,0].legend([f'layer $h^{i+1}$' for i in layers])\n",
    "fig.suptitle('Null distribution of #clusters')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "finnish-tenant",
   "metadata": {},
   "source": [
    "### Plot Q score vs hyper, separate panel per assoc method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-bulgaria",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "color0 = plot_format({'assoc': all_assoc[0]})['color']\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12,6))\n",
    "\n",
    "for i, a in enumerate(all_assoc):\n",
    "    r, c = i//4, i%4\n",
    "    plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l1':0.,'drop':0.,'assoc':a}),\n",
    "                         x='l2', y='score', ax=ax[r,c], std_hyper=True,\n",
    "                         errbar_args={'marker':'o', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['score'].max()], 'legend': False, 'ylabel': None, 'xlabel': None},\n",
    "                         df_null=df_slice(mod_df_null, {'assoc':a}))\n",
    "    plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l2':1e-5,'drop':0.,'assoc':a}),\n",
    "                         x='l1', y='score', ax=ax[r,c], std_hyper=True,\n",
    "                         errbar_args={'marker':'D', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['score'].max()], 'legend': False, 'ylabel': None, 'xlabel': None})\n",
    "    plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l2':1e-5,'l1':0.,'assoc':a}),\n",
    "                         x='drop', y='score', ax=ax[r,c], std_hyper=True,\n",
    "                         errbar_args={'marker':'*', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['score'].max()], 'legend': False, 'ylabel': None, 'xlabel': None})\n",
    "    ax[r,c].set_title(relabel[a])\n",
    "    ax[r,c].set_ylabel('modularity score (Q)' if c==0 else None)\n",
    "    ax[r,c].set_xlabel('% regularization' if r==1 else None)\n",
    "\n",
    "ax[0,0].legend(loc='lower right', handles=[Line2D([0],[0], color=color0, markeredgecolor='k', marker='o', label='$L_2$'),\n",
    "                                          Line2D([0],[0], color=color0, markeredgecolor='k', marker='D', label='$L_1$'),\n",
    "                                          Line2D([0],[0], color=color0, markeredgecolor='k', marker='*', label='drop')])\n",
    "\n",
    "fig.suptitle('Modularity scores')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"figures/{config[\"log-dir-prefix\"].replace('-','_')}_q_vs_reg.svg\")\n",
    "plt.show()\n",
    "\n",
    "# Do it again but for #clusters\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12,6))\n",
    "\n",
    "for i, a in enumerate(all_assoc):\n",
    "    r, c = i//4, i%4\n",
    "    plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l1':0.,'drop':0.,'assoc':a}),\n",
    "                         x='l2', y='num_clusters', ax=ax[r,c], std_hyper=True,\n",
    "                         errbar_args={'marker':'o', 'markeredgecolor':'k', 'ylim': [0, mod_df_all['num_clusters'].max()], 'legend': False, 'ylabel': None, 'xlabel': None},\n",
    "                         df_null=df_slice(mod_df_null, {'assoc':a}))\n",
    "    plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l2':1e-5,'drop':0.,'assoc':a}),\n",
    "                         x='l1', y='num_clusters', ax=ax[r,c], std_hyper=True,\n",
    "                         errbar_args={'marker':'D', 'markeredgecolor':'k', 'ylim': [0, mod_df_all['num_clusters'].max()], 'legend': False, 'ylabel': None, 'xlabel': None})\n",
    "    plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l2':1e-5,'l1':0.,'assoc':a}),\n",
    "                         x='drop', y='num_clusters', ax=ax[r,c], std_hyper=True,\n",
    "                         errbar_args={'marker':'*', 'markeredgecolor':'k', 'ylim': [0, mod_df_all['num_clusters'].max()], 'legend': False, 'ylabel': None, 'xlabel': None})\n",
    "    ax[r,c].set_title(relabel[a])\n",
    "    ax[r,c].set_ylabel('#clusters' if c==0 else None)\n",
    "    ax[r,c].set_xlabel('% regularization' if r==1 else None)\n",
    "\n",
    "ax[0,0].legend(loc='lower right', handles=[Line2D([0],[0], color=color0, markeredgecolor='k', marker='o', label='$L_2$'),\n",
    "                                          Line2D([0],[0], color=color0, markeredgecolor='k', marker='D', label='$L_1$'),\n",
    "                                          Line2D([0],[0], color=color0, markeredgecolor='k', marker='*', label='drop')])\n",
    "\n",
    "fig.suptitle('#Clusters')\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"figures/{config[\"log-dir-prefix\"].replace('-','_')}_numc_vs_reg.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-ultimate",
   "metadata": {},
   "source": [
    "### Repeat the above, now additionally splitting by layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-watts",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "color0 = plot_format({'assoc': all_assoc[0]})['color']\n",
    "\n",
    "for l in layers:\n",
    "    fig, ax = plt.subplots(2, 4, figsize=(12,6))\n",
    "    for i, a in enumerate(all_assoc):\n",
    "        r, c = i//4, i%4\n",
    "        plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l1':0.,'drop':0.,'assoc':a,'layer':l}),\n",
    "                             x='l2', y='score', ax=ax[r,c], std_hyper=True,\n",
    "                             errbar_args={'marker':'o', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['score'].max()], 'legend': False, 'ylabel': None, 'xlabel': None},\n",
    "                             df_null=df_slice(mod_df_null, {'assoc':a,'layer':l}))\n",
    "        plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l2':1e-5,'drop':0.,'assoc':a,'layer':l}),\n",
    "                             x='l1', y='score', ax=ax[r,c], std_hyper=True,\n",
    "                             errbar_args={'marker':'D', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['score'].max()], 'legend': False, 'ylabel': None, 'xlabel': None})\n",
    "        plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l2':1e-5,'l1':0.,'assoc':a,'layer':l}),\n",
    "                             x='drop', y='score', ax=ax[r,c], std_hyper=True,\n",
    "                             errbar_args={'marker':'*', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['score'].max()], 'legend': False, 'ylabel': None, 'xlabel': None})\n",
    "        ax[r,c].set_title(relabel[a])\n",
    "        ax[r,c].set_ylabel('modularity score (Q)' if c==0 else None)\n",
    "        ax[r,c].set_xlabel('% regularization' if r==1 else None)\n",
    "\n",
    "    ax[0,0].legend(loc='lower right', handles=[Line2D([0],[0], color=color0, markeredgecolor='k', marker='o', label='$L_2$'),\n",
    "                                              Line2D([0],[0], color=color0, markeredgecolor='k', marker='D', label='$L_1$'),\n",
    "                                              Line2D([0],[0], color=color0, markeredgecolor='k', marker='*', label='drop')])\n",
    "    \n",
    "    fig.suptitle(f'Modularity scores (Layer $h^{l+1}$)')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"figures/{config[\"log-dir-prefix\"].replace('-','_')}_q_vs_reg_h{l+1}.svg\")\n",
    "    plt.show()\n",
    "\n",
    "    # Do it again but for #clusters\n",
    "    fig, ax = plt.subplots(2, 4, figsize=(12,6))\n",
    "    for i, a in enumerate(all_assoc):\n",
    "        r, c = i//4, i%4\n",
    "        plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l1':0.,'drop':0.,'assoc':a,'layer':l}),\n",
    "                             x='l2', y='num_clusters', ax=ax[r,c], std_hyper=True,\n",
    "                             errbar_args={'marker':'o', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['num_clusters'].max()], 'legend': False, 'ylabel': None, 'xlabel': None},\n",
    "                             df_null=df_slice(mod_df_null, {'assoc':a,'layer':l}))\n",
    "        plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l2':1e-5,'drop':0.,'assoc':a,'layer':l}),\n",
    "                             x='l1', y='num_clusters', ax=ax[r,c], std_hyper=True,\n",
    "                             errbar_args={'marker':'D', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['num_clusters'].max()], 'legend': False, 'ylabel': None, 'xlabel': None})\n",
    "        plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l2':1e-5,'l1':0.,'assoc':a,'layer':l}),\n",
    "                             x='drop', y='num_clusters', ax=ax[r,c], std_hyper=True,\n",
    "                             errbar_args={'marker':'*', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['num_clusters'].max()], 'legend': False, 'ylabel': None, 'xlabel': None})\n",
    "        ax[r,c].set_title(relabel[a])\n",
    "        ax[r,c].set_ylabel('#clusters' if c==0 else None)\n",
    "        ax[r,c].set_xlabel('% regularization' if r==1 else None)\n",
    "\n",
    "    ax[0,0].legend(loc='lower right', handles=[Line2D([0],[0], color=color0, markeredgecolor='k', marker='o', label='$L_2$'),\n",
    "                                              Line2D([0],[0], color=color0, markeredgecolor='k', marker='D', label='$L_1$'),\n",
    "                                              Line2D([0],[0], color=color0, markeredgecolor='k', marker='*', label='drop')])\n",
    "    \n",
    "    fig.suptitle(f'#Clusters (Layer $h^{l+1}$)')\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"figures/{config['log-dir-prefix'].replace('-','_')}_numc_vs_reg_h{l+1}.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "first-discount",
   "metadata": {},
   "source": [
    "### Q and num clusters vs depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture --no-display\n",
    "color0 = plot_format({'assoc': all_assoc[0]})['color']\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12,6))\n",
    "\n",
    "for i, a in enumerate(all_assoc):\n",
    "    r, c = i//4, i%4\n",
    "    plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l1':0.,'drop':0.,'assoc':a,'l2':max_l2}),\n",
    "                         x='layer', y='score', ax=ax[r,c],\n",
    "                         errbar_args={'marker':'o', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['score'].max()], 'legend': False, 'ylabel': None, 'xlabel': None},\n",
    "                         df_null=df_slice(mod_df_null, {'assoc':a}), null_x=True)\n",
    "    plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l2':1e-5,'drop':0.,'assoc':a,'l1':max_l1}),\n",
    "                         x='layer', y='score', ax=ax[r,c],\n",
    "                         errbar_args={'marker':'D', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['score'].max()], 'legend': False, 'ylabel': None, 'xlabel': None})\n",
    "    plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l2':1e-5,'l1':0.,'assoc':a,'drop':max_drop}),\n",
    "                         x='layer', y='score', ax=ax[r,c],\n",
    "                         errbar_args={'marker':'*', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['score'].max()], 'legend': False, 'ylabel': None, 'xlabel': None})\n",
    "    ax[r,c].set_title(relabel[a])\n",
    "    ax[r,c].set_ylabel('modularity score (Q)' if c==0 else None)\n",
    "    ax[r,c].set_xlabel('Layer' if r==1 else None)\n",
    "\n",
    "ax[0,0].legend(loc='upper left', handles=[Line2D([0],[0], color=color0, markeredgecolor='k', marker='o', label='$L_2$'),\n",
    "                                          Line2D([0],[0], color=color0, markeredgecolor='k', marker='D', label='$L_1$'),\n",
    "                                          Line2D([0],[0], color=color0, markeredgecolor='k', marker='*', label='drop')])\n",
    "\n",
    "fig.suptitle('Modularity scores')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"figures/{config['log-dir-prefix'].replace('-','_')}_q_vs_depth.svg\")\n",
    "plt.show()\n",
    "\n",
    "# Do it again but for #clusters\n",
    "fig, ax = plt.subplots(2, 4, figsize=(12,6))\n",
    "\n",
    "for i, a in enumerate(all_assoc):\n",
    "    r, c = i//4, i%4\n",
    "    plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l1':0.,'drop':0.,'assoc':a,'l2':max_l2}),\n",
    "                         x='layer', y='num_clusters', ax=ax[r,c],\n",
    "                         errbar_args={'marker':'o', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['num_clusters'].max()], 'legend': False, 'ylabel': None, 'xlabel': None},\n",
    "                         df_null=df_slice(mod_df_null, {'assoc':a}), null_x=True)\n",
    "    plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l2':1e-5,'drop':0.,'assoc':a,'l1':max_l1}),\n",
    "                         x='layer', y='num_clusters', ax=ax[r,c],\n",
    "                         errbar_args={'marker':'D', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['num_clusters'].max()], 'legend': False, 'ylabel': None, 'xlabel': None})\n",
    "    plot_vs_hyper_splits(df=df_slice(mod_df_all, {'l2':1e-5,'l1':0.,'assoc':a,'drop':max_drop}),\n",
    "                         x='layer', y='num_clusters', ax=ax[r,c],\n",
    "                         errbar_args={'marker':'*', 'markeredgecolor':'k', 'ylim': [0.0, mod_df_all['num_clusters'].max()], 'legend': False, 'ylabel': None, 'xlabel': None})\n",
    "    ax[r,c].set_title(relabel[a])\n",
    "    ax[r,c].set_ylabel('#clusters' if c==0 else None)\n",
    "    ax[r,c].set_xlabel('Layer' if r==1 else None)\n",
    "\n",
    "ax[0,0].legend(loc='upper left', handles=[Line2D([0],[0], color=color0, markeredgecolor='k', marker='o', label='$L_2$'),\n",
    "                                          Line2D([0],[0], color=color0, markeredgecolor='k', marker='D', label='$L_1$'),\n",
    "                                          Line2D([0],[0], color=color0, markeredgecolor='k', marker='*', label='drop')])\n",
    "\n",
    "fig.suptitle('Modularity scores')\n",
    "\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"figures/{config['log-dir-prefix'].replace('-','_')}_numc_vs_depth.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick some more assocation matrices to look at based on what we see here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "healthy-strap",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_plot_assoc(savedir, model_spec, layer, assoc, ax=None, title=None):\n",
    "    ax=ax or plt.gca()\n",
    "    demo_ckpt = Path(savedir) / LitWrapper(**model_spec).get_uid() / 'weights' / 'best.ckpt'\n",
    "    info = torch.load(demo_ckpt, map_location='cpu')\n",
    "    adj, clust = info[\"modules\"][assoc][layer]['adj'], info[\"modules\"][assoc][layer]['clusters']\n",
    "    isrt = sort_by_cluster(clust, remove_dead=True)\n",
    "    sizes = clust.sum(0)\n",
    "    plot_assoc(adj[:,isrt][isrt,:], ax=ax, lines_at=sizes, colorbar=True)\n",
    "    ttl = f\"Q = {girvan_newman(adj,clust):.3f}\"\n",
    "    if title is not None:\n",
    "        ttl += \"\\n\" + title\n",
    "    ax.set_title(ttl)\n",
    "\n",
    "\n",
    "base_spec = {'dataset': config['dataset'], 'task': 'sup', 'l2': 1e-5, 'l1':0.0, 'drop':0.0, 'run': 0}\n",
    "\n",
    "fig, ax = plt.subplots(1,3,figsize=(12,3))\n",
    "for i, d in enumerate([0.1, 0.3, 0.5]):\n",
    "    load_and_plot_assoc(f\"logs-{config['log-dir-prefix']}-drop\", merge_dicts(base_spec, {'drop': d}), ax=ax[i], layer=0, assoc='forward_cov_norm')\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"figures/{config['log-dir-prefix'].replace('-','_')}_example_assoc_vs_drop.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "raised-differential",
   "metadata": {},
   "source": [
    "# How well do methods agree?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hungarian-study",
   "metadata": {},
   "source": [
    "# Module alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-czech",
   "metadata": {},
   "outputs": [],
   "source": [
    "# assoc = [\"forward_cov\", \"backward_hess\", \"forward_jac\", \"backward_jac\"]\n",
    "assoc = [\"forward_cov\", \"forward_jac\", \"backward_jac\"]\n",
    "norms = [\"\", \"_norm\"]\n",
    "fields = [\"score\", \"p\", \"version\",\"rmi\",\"vi\",\"rmi_norm\",\"vi_norm\",\"element_sim\",\"adj_spearman_r\",\"adj_spearman_p\",\"transfer_AaPb\",\"transfer_AbPa\",\"transfer_AaPb_norm\",\"transfer_AbPa_norm\"]\n",
    "layers = list(range(config['layers']))\n",
    "sparsenesses = [\"\"]#+[f\".{f:.2f}\" for f in [.05,.1,.2,.3,.4,.5]]\n",
    "\n",
    "metrics = set()\n",
    "for (io1, nm1, io2, nm2, sp, lay, field) in itertools.product(assoc, norms, assoc, norms, sparsenesses, layers, fields):\n",
    "    assoc_a, assoc_b = min([io1+nm1, io2+nm2]), max([io1+nm1, io2+nm2])\n",
    "    metrics.add(f\"align.{assoc_a}:{assoc_b}{sp}.{lay}.{field}\")\n",
    "metrics.add(\"test_acc\")\n",
    "\n",
    "align_df_l2 = load_data_as_table(\n",
    "    generate_model_specs({'dataset': config['dataset'], 'task': 'sup', 'l1': 0.0, 'drop': 0.0,\n",
    "                          'l2': config['regularization']['l2'], 'run': range(config['seeds']), 'checkpoint': 'best'}),\n",
    "    metrics,\n",
    "    log_dir=Path(f'logs-{config[\"log-dir-prefix\"]}-l2'))\n",
    "\n",
    "align_df_l1 = load_data_as_table(\n",
    "    generate_model_specs({'dataset': config['dataset'], 'task': 'sup', 'l2': 1e-5, 'drop': 0.0,\n",
    "                          'l1': config['regularization']['l1'], 'run': range(config['seeds']), 'checkpoint': 'best'}),\n",
    "    metrics,\n",
    "    log_dir=Path(f'logs-{config[\"log-dir-prefix\"]}-l1'))\n",
    "\n",
    "align_df_drop = load_data_as_table(\n",
    "    generate_model_specs({'dataset': config['dataset'], 'task': 'sup', 'l2': 1e-5, 'l1': 0.0,\n",
    "                          'drop': config['regularization']['drop'], 'run': range(config['seeds']), 'checkpoint': 'best'}),\n",
    "    metrics,\n",
    "    log_dir=Path(f'logs-{config[\"log-dir-prefix\"]}-drop'))\n",
    "\n",
    "align_df_null = load_data_as_table(\n",
    "    generate_model_specs({'dataset': config['dataset'], 'task': 'sup', 'l2': 0., 'drop': 0., 'l1': 0.,\n",
    "                          'run': range(1000,1100), 'checkpoint': 'dummy.ckpt'}),\n",
    "    metrics,\n",
    "    log_dir=Path(f'logs-{config[\"log-dir-prefix\"]}-dummy'))\n",
    "\n",
    "align_df_all = pd.concat([align_df_l2, align_df_l1, align_df_drop])\n",
    "\n",
    "# Drop alignment metrics wherever test_acc < test_acc_cutoff\n",
    "align_df_l2.loc[align_df_l2[\"test_acc\"] < test_acc_cutoff, fields] = float('nan')\n",
    "align_df_l1.loc[align_df_l1[\"test_acc\"] < test_acc_cutoff, fields] = float('nan')\n",
    "align_df_drop.loc[align_df_drop[\"test_acc\"] < test_acc_cutoff, fields] = float('nan')\n",
    "align_df_all.loc[align_df_all[\"test_acc\"] < test_acc_cutoff, fields] = float('nan')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute more numerically stable 'transfer_norm' field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eps = 1e-3\n",
    "\n",
    "mod_index_keys = ['dataset', 'task', 'l1', 'drop', 'l2', 'run', 'checkpoint', 'layer']\n",
    "\n",
    "new_transfer_norm_AaPb = []\n",
    "new_transfer_norm_AbPa = []\n",
    "old_transfer_norm_AaPb = []\n",
    "old_transfer_norm_AbPa = []\n",
    "new_transfer_norm_combo = []\n",
    "for idx, row in align_df_all.iterrows():\n",
    "    mod_slice_a = {k: row[k] for k in mod_index_keys}\n",
    "    mod_slice_a['assoc'] = row['assoc_a']\n",
    "    mod_row_a = df_slice(mod_df_all, mod_slice_a)\n",
    "    \n",
    "    mod_slice_b = {k: row[k] for k in mod_index_keys}\n",
    "    mod_slice_b['assoc'] = row['assoc_b']\n",
    "    mod_row_b = df_slice(mod_df_all, mod_slice_b)\n",
    "    \n",
    "    new_transfer_norm_AaPb.append(row['transfer_AaPb'] / (mod_row_a['score'] + eps))\n",
    "    new_transfer_norm_AbPa.append(row['transfer_AbPa'] / (mod_row_b['score'] + eps))\n",
    "    old_transfer_norm_AaPb.append(row['transfer_AaPb_norm'])\n",
    "    old_transfer_norm_AbPa.append(row['transfer_AbPa_norm'])\n",
    "    new_transfer_norm_combo.append((row['transfer_AaPb'] + row['transfer_AbPa']) / (mod_row_a['score'] + mod_row_b['score'] + eps))\n",
    "\n",
    "new_transfer_norm_AaPb = np.array(new_transfer_norm_AaPb)\n",
    "new_transfer_norm_AbPa = np.array(new_transfer_norm_AbPa)\n",
    "old_transfer_norm_AaPb = np.array(old_transfer_norm_AaPb)\n",
    "old_transfer_norm_AbPa = np.array(old_transfer_norm_AbPa)\n",
    "new_transfer_norm_combo = np.array(new_transfer_norm_combo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.scatter(old_transfer_norm_AaPb, new_transfer_norm_AaPb, marker='.')\n",
    "plt.scatter(old_transfer_norm_AbPa, new_transfer_norm_AbPa, marker='.')\n",
    "plt.scatter((old_transfer_norm_AbPa + old_transfer_norm_AaPb)/2, new_transfer_norm_combo, marker='.')\n",
    "plt.xlim([-1,2])\n",
    "plt.ylim([-1,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine two 'transfer' fields together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in [align_df_null, align_df_l2, align_df_l1, align_df_drop, align_df_all]:\n",
    "    df['transfer'] = (df['transfer_AaPb'] + df['transfer_AbPa']) / 2\n",
    "    df['transfer_norm'] = (df['transfer_AaPb_norm'] + df['transfer_AbPa_norm']) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tracked-compression",
   "metadata": {},
   "outputs": [],
   "source": [
    "def alignment_stats(df, value='score'):\n",
    "    pvt = df.pivot(index=('l1','l2','drop','run','layer'), columns=['assoc_a', 'assoc_b'], values=[value])\n",
    "    average = pvt.mean()[value]\n",
    "    \n",
    "    assoc = df['assoc_a'].unique()\n",
    "    assoc.sort()\n",
    "    out = np.zeros((len(assoc), len(assoc)))\n",
    "    for i, a in enumerate(assoc):\n",
    "        for j, b in enumerate(assoc):\n",
    "            out[i,j] = average.loc[(min(a,b),max(a,b))]\n",
    "    return out, assoc\n",
    "\n",
    "rg = LinearSegmentedColormap('redgreen',\n",
    "                             {'red': [(0.,1.,1.),(.5,0.,0.),(1.,0.,0.)],\n",
    "                              'green': [(0.,0.,0.),(.5,0.,0.),(1.,1.,1.)],\n",
    "                              'blue': [(0.,0.,0.),(1.,0.,0.)]})\n",
    "\n",
    "def plot_alignment_stats(df, ax=None, relative=None, textlabel=True, value='score'):\n",
    "    scores, labels = alignment_stats(df, value=value)\n",
    "    signif, _ = alignment_stats(df, value='p')\n",
    "    \n",
    "    if relative is not None:\n",
    "        baseline_scores, _ = alignment_stats(relative, value=value)\n",
    "        scores -= baseline_scores\n",
    "        vmin, vmax, cmap = -np.max(np.abs(scores)), np.max(np.abs(scores)), rg\n",
    "    else:\n",
    "        vmin, vmax, cmap = np.min(np.abs(scores)), np.max(np.abs(scores)), 'viridis'\n",
    "\n",
    "    ax = ax or plt.gca()\n",
    "    img = ax.matshow(scores, vmin=vmin, vmax=vmax, cmap=cmap)\n",
    "    plt.colorbar(img, ax=ax, fraction=.047)\n",
    "    ax.set_xticks(range(len(labels)))\n",
    "    ax.set_yticks(range(len(labels)))\n",
    "    ax.set_xticklabels([])\n",
    "    ax.set_yticklabels([])\n",
    "    barsize = 4.7 if textlabel else 0.8\n",
    "    for i, a in enumerate(labels):\n",
    "        ax.add_patch(Rectangle((i-.4,-barsize-.6),.8,barsize,fill=True,clip_on=False,color=plot_format({'assoc':a})['color']))\n",
    "        ax.add_patch(Rectangle((-barsize-.6,i-.4),barsize,.8,fill=True,clip_on=False,color=plot_format({'assoc':a})['color']))\n",
    "    if textlabel:\n",
    "        ax.set_xticklabels([relabel[l] for l in labels], rotation=90, color='w')\n",
    "        ax.set_yticklabels([relabel[l] for l in labels], color='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-business",
   "metadata": {},
   "source": [
    "How well do different alignment scores agree with each other (corr separately per regularization/layer/similarity method, then averaged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-leadership",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment_fields = ['score', 'rmi', 'rmi_norm', 'vi', 'vi_norm', 'element_sim', 'adj_spearman_r', 'transfer', 'transfer_norm']\n",
    "avg_corr, n = np.zeros((len(alignment_fields), len(alignment_fields))), np.zeros((len(alignment_fields), len(alignment_fields)))\n",
    "for _, grp in align_df_all.groupby(['l2', 'l1', 'drop', 'layer', 'run']):\n",
    "    corr = grp[alignment_fields].corr().to_numpy()\n",
    "    valid = ~np.isnan(corr)\n",
    "    n[valid] += 1\n",
    "    avg_corr[valid] += (corr[valid] - avg_corr[valid]) / n[valid]\n",
    "plt.imshow(np.abs(avg_corr))\n",
    "remap_align = {\n",
    "    'score': 'greedy',\n",
    "    'rmi': 'rmi',\n",
    "    'rmi_norm': 'rmi_norm',\n",
    "    'vi': 'vi',\n",
    "    'vi_norm': 'vi_norm',\n",
    "    'element_sim': 'element_sim',\n",
    "    'adj_spearman_r': 'RSA',\n",
    "    'transfer': 'transfer',\n",
    "    'transfer_norm': 'transfer_norm'\n",
    "}\n",
    "plt.xticks(*zip(*enumerate([remap_align[f] for f in alignment_fields])), rotation='vertical')\n",
    "plt.yticks(*zip(*enumerate([remap_align[f] for f in alignment_fields])))\n",
    "plt.title(\"Spearman correlation of different 'alignment' tests\")\n",
    "plt.colorbar()\n",
    "\n",
    "plt.gcf().tight_layout()\n",
    "plt.gcf().savefig(f\"figures/{config['log-dir-prefix'].replace('-','_')}_corr_between_alignment_methods.svg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intelligent-dublin",
   "metadata": {},
   "source": [
    "Pick the metric we'll use going forward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "naughty-cowboy",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALIGNMENT_METRIC = 'element_sim'\n",
    "# ALIGNMENT_METRIC = 'transfer'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artificial-declaration",
   "metadata": {},
   "source": [
    "Show alignment scores in untrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "supposed-stake",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_alignment_stats(align_df_null, textlabel=True, value=ALIGNMENT_METRIC)\n",
    "plt.savefig(f\"figures/{config['log-dir-prefix'].replace('-','_')}_alignment_{ALIGNMENT_METRIC}_untrained.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geological-compatibility",
   "metadata": {},
   "source": [
    "### 1st summary plot: just look at average alignment across everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-apache",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2,figsize=(11.5, 5.75))\n",
    "plot_alignment_stats(align_df_all, relative=None, ax=ax[0], textlabel=True, value=ALIGNMENT_METRIC)\n",
    "plot_alignment_stats(align_df_all, relative=align_df_null, ax=ax[1], textlabel=True, value=ALIGNMENT_METRIC)\n",
    "fig.tight_layout()\n",
    "fig.savefig(f\"figures/{config['log-dir-prefix'].replace('-','_')}_align_summary_{ALIGNMENT_METRIC}.svg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graphic-arthritis",
   "metadata": {},
   "source": [
    "### Show alignment stats per regularization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bottom-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_l2vals = [val for val in config['regularization']['l2'] if val <= max_l2]\n",
    "l2vals = [valid_l2vals[int(i)] for i in np.round(np.linspace(0, len(valid_l2vals)-1, 4))]\n",
    "valid_l1vals = [val for val in config['regularization']['l1'] if val <= max_l1]\n",
    "l1vals = [valid_l1vals[int(i)] for i in np.round(np.linspace(0, len(valid_l1vals)-1, 4))]\n",
    "valid_dropvals = [val for val in config['regularization']['drop'] if val <= max_drop]\n",
    "dropvals = [valid_dropvals[int(i)] for i in np.round(np.linspace(0, len(valid_dropvals)-1, 4))]\n",
    "\n",
    "for rel in [None, align_df_null]:\n",
    "    fig, ax = plt.subplots(3,4,figsize=(12,9))\n",
    "    for i, l2 in enumerate(l2vals):\n",
    "        plot_alignment_stats(df_slice(align_df_all, {'l2':l2, 'l1':0., 'drop':0.}), relative=rel, ax=ax[0,i], textlabel=False, value=ALIGNMENT_METRIC)\n",
    "        # ax[0,i].set_title(f'$L_2$={int(regularization_amount(\"l2\",l2)*100):d}%\\n')\n",
    "        ax[0,i].set_title(f'$L_2$={l2:.2e}\\n')\n",
    "    for i, l1 in enumerate(l1vals):\n",
    "        plot_alignment_stats(df_slice(align_df_all, {'l1':l1}), relative=rel, ax=ax[1,i], textlabel=False, value=ALIGNMENT_METRIC)\n",
    "        # ax[1,i].set_title(f'$L_1$={int(regularization_amount(\"l1\",l1)*100):d}%\\n')\n",
    "        ax[1,i].set_title(f'$L_1$={l1:.2e}\\n')\n",
    "    for i, drop in enumerate(dropvals):\n",
    "        plot_alignment_stats(df_slice(align_df_all, {'drop':drop}), relative=rel, ax=ax[2,i], textlabel=False, value=ALIGNMENT_METRIC)\n",
    "        # ax[2,i].set_title(f'drop={int(regularization_amount(\"drop\",drop)*100):d}%\\n')\n",
    "        ax[2,i].set_title(f'drop={drop:.2f}\\n')\n",
    "\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(f\"figures/{config['log-dir-prefix'].replace('-', '_')}_align_vs_reg_{ALIGNMENT_METRIC}{'_delta' if rel is not None else ''}.svg\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "random-behalf",
   "metadata": {},
   "source": [
    "### Show alignment stats per regularization method per layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-fence",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for lay in layers:\n",
    "    for rel in [None, align_df_null]:\n",
    "        if rel is not None:\n",
    "            rel = df_slice(rel, {'layer':0})\n",
    "        fig, ax = plt.subplots(3,4,figsize=(12,9))\n",
    "        for i, l2 in enumerate(l2vals):\n",
    "            plot_alignment_stats(df_slice(align_df_all, {'layer':lay, 'l2':l2, 'l1':0., 'drop':0.}), relative=rel, ax=ax[0,i], textlabel=False, value=ALIGNMENT_METRIC)\n",
    "            # ax[0,i].set_title(f'$L_2$={int(regularization_amount(\"l2\",l2)*100):d}%\\n')\n",
    "            ax[0,i].set_title(f'$L_2$={l2:.2e}\\n')\n",
    "        for i, l1 in enumerate(l1vals):\n",
    "            plot_alignment_stats(df_slice(align_df_all, {'layer':lay, 'l1':l1}), relative=rel, ax=ax[1,i], textlabel=False, value=ALIGNMENT_METRIC)\n",
    "            # ax[1,i].set_title(f'$L_1$={int(regularization_amount(\"l1\",l1)*100):d}%\\n')\n",
    "            ax[1,i].set_title(f'$L_1$={l1:.2e}\\n')\n",
    "        for i, drop in enumerate(dropvals):\n",
    "            plot_alignment_stats(df_slice(align_df_all, {'layer':lay, 'drop':drop}), relative=rel, ax=ax[2,i], textlabel=False, value=ALIGNMENT_METRIC)\n",
    "            # ax[2,i].set_title(f'drop={int(regularization_amount(\"drop\",drop)*100):d}%\\n')\n",
    "            ax[2,i].set_title(f'drop={drop:.2f}\\n')\n",
    "\n",
    "        fig.suptitle(f'Layer $h^{lay+1}$')\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(f'figures/{config[\"log-dir-prefix\"].replace(\"-\", \"_\")}_align_h{lay+1}_vs_reg_{ALIGNMENT_METRIC}{\"_delta\" if rel is not None else \"\"}.svg')\n",
    "        plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
